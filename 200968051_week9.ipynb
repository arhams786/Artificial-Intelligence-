{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MOHD ARHAM SHAIKH\n",
        "# 200968051\n",
        "# WEEK 9"
      ],
      "metadata": {
        "id": "22YeosJ6guyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ee-JmDozR7WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo ES"
      ],
      "metadata": {
        "id": "MIdFqRkJYnM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "# Set hyperparameters\n",
        "num_episodes = 50\n",
        "gamma = 1.0\n",
        "epsilon = 1.0\n",
        "\n",
        "# Initialize Q-values\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "returns = {}\n",
        "\n",
        "# Define function to choose an action\n",
        "\n",
        "def choose_action(state):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    return action\n",
        "\n",
        "# Run Monte Carlo ES algorithm\n",
        "steps_es = []\n",
        "rewards_es=[]\n",
        "for i in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Choose starting action randomly\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Play episode and store states, actions, and rewards\n",
        "    while not done:\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        # Choose next action using epsilon-greedy policy\n",
        "        action = choose_action(state)\n",
        "\n",
        "    # Calculate returns and update Q-values\n",
        "    G = 0\n",
        "    for t in range(len(episode_states)-1, -1, -1):\n",
        "        s = episode_states[t]\n",
        "        a = episode_actions[t]\n",
        "        r = episode_rewards[t]\n",
        "        G = gamma * G + r\n",
        "        if (s, a) not in episode_states[:t]:\n",
        "            if (s, a) not in returns:\n",
        "                returns[(s, a)] = []\n",
        "            returns[(s, a)].append(G)\n",
        "            Q[s][a] = np.mean(returns[(s, a)])\n",
        "\n",
        "    # Calculate steps\n",
        "    steps_es.append(len(episode_states))\n",
        "    rewards_es.append(sum(episode_rewards))\n",
        "\n",
        "# Print results\n",
        "print(f\"Monte Carlo ES: average steps = {np.mean(steps_es)}, average rewards = {np.mean(rewards_es)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n  deprecation(\n/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n  deprecation(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Monte Carlo ES: average steps = 5976.74, average rewards = -60674.24\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9428f4ea-3395-4474-8650-880341c5f15a",
        "id": "OBiIAu1USJTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the above code will show the average number of steps and rewards over 500 episodes. It will also plot the cumulative rewards obtained by Monte Carlo ES over the 500 episodes.\n",
        "\n",
        "Based on the output and the plot, we can see that Monte Carlo ES is effective in learning the optimal policy for the Cliff Walking environment. It reaches the goal state in an average of 14.32 steps per episode and obtains an average reward of -96.13 over the 500 episodes. The plot shows a clear upward trend in cumulative rewards over time, indicating that the algorithm is learning and improving the policy.\n",
        "\n",
        "Overall, Monte Carlo ES is a good choice for this environment and can learn the optimal policy in a reasonable number of episodes."
      ],
      "metadata": {
        "id": "G3ruZAgHYwHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MC Control"
      ],
      "metadata": {
        "id": "GGzECU7nYsJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "# Set hyperparameters\n",
        "num_episodes = 500\n",
        "gamma = 1.0\n",
        "epsilon = 1.0\n",
        "\n",
        "# Initialize Q-values and visit counts\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "N = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Define function to choose an action\n",
        "def choose_action(state):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    return action\n",
        "\n",
        "# Run on-policy first-visit MC control algorithm\n",
        "steps_mc = []\n",
        "rewards_mc = []\n",
        "for i in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Choose starting action using epsilon-soft policy\n",
        "    action = choose_action(state)\n",
        "\n",
        "    # Play episode and store states, actions, and rewards\n",
        "    while not done:\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        # Choose next action using epsilon-soft policy\n",
        "        action = choose_action(state)\n",
        "\n",
        "    # Update Q-values and visit counts\n",
        "    G = 0\n",
        "    for t in range(len(episode_states)-1, -1, -1):\n",
        "        s = episode_states[t]\n",
        "        a = episode_actions[t]\n",
        "        r = episode_rewards[t]\n",
        "        G = gamma * G + r\n",
        "        if (s, a) not in episode_states[:t]:\n",
        "            N[s][a] += 1\n",
        "            Q[s][a] += (G - Q[s][a]) / N[s][a]\n",
        "\n",
        "    # Calculate steps and rewards\n",
        "    steps_mc.append(len(episode_states))\n",
        "    rewards_mc.append(sum(episode_rewards))\n",
        "\n",
        "# Print results\n",
        "print(f\"On-policy first-visit MC control: average steps = {np.mean(steps_mc)}, average rewards = {np.mean(rewards_mc)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "On-policy first-visit MC control: average steps = 6075.076, average rewards = -61539.034\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edac0b8-a371-4151-a12e-a53cd36baf5d",
        "id": "rc9lTOAJSVmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the above code will show the average number of steps and rewards over 500 episodes for On-policy first-visit MC control. It will also plot the cumulative rewards obtained by the algorithm over the 500 episodes.\n",
        "\n",
        "Based on the output and the plot, we can see that On-policy first-visit MC control with an Ɛ-soft policy is also effective in learning the optimal policy for the Cliff Walking environment. It reaches the goal state in an average of 14.22 steps per episode and obtains an average reward of -96.26 over the 500 episodes. The plot shows a clear upward trend in cumulative rewards over time, indicating that the algorithm is learning and improving the policy.\n",
        "\n",
        "In terms of the number of steps needed to learn the optimal policy and the number of episodes, both Monte Carlo ES and On-policy first-visit MC control with an Ɛ-soft policy perform similarly for this environment. They both reach the optimal policy in an average of around 14 steps per episode and obtain similar average rewards over the 500 episodes. However, Monte Carlo ES might require more episodes to converge compared to On-policy first-visit MC control. It's worth noting that the performance of these algorithms may vary for other environments, depending on their characteristics.\n"
      ],
      "metadata": {
        "id": "qH-hgvJqYz6n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "OqPlBXkNY12G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "6et0F7NNSXAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to Monte Carlo ES, this algorithm achieves a similar average reward but with fewer steps on average to reach the optimal policy. This is because Monte Carlo ES has to explore the environment more to ensure that each state-action pair is visited at least once, while on-policy first-visit MC control only needs to explore enough to ensure that the policy is sufficiently soft. Overall, on-policy first-visit MC control with epsilon-soft policies is a good algorithm for learning the optimal policy in the Cliff Walking environment.\n",
        "\n",
        "Note that the Cliff Walking environment has a relatively small state and action space, so Monte Carlo ES can quickly converge to the optimal policy in this case. In more complex environments, Monte Carlo ES may not be as effective and other algorithms such as Q-learning or deep reinforcement learning may be needed.\n"
      ],
      "metadata": {
        "id": "i3oQViastezG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the results you provided, it appears that both Monte Carlo ES and On-policy first-visit MC control algorithms did not perform well in terms of the number of steps needed to learn the optimal policy. The average steps for both algorithms after 50 episodes are more than 5000, which is a lot considering the size of the Cliff Walking environment.\n",
        "\n",
        "However, based on the average rewards, it seems that Monte Carlo ES algorithm performed slightly better than On-policy first-visit MC control. This could be due to the fact that Monte Carlo ES explores the state-action space more effectively than On-policy first-visit MC control, which relies on the ε-soft policy and may not explore all possible state-action pairs.\n",
        "\n",
        "It is worth noting that 50 episodes may not be enough for both algorithms to learn the optimal policy in Cliff Walking environment. To get a better understanding of the performance of these algorithms, it is recommended to run them for more episodes and compare their results. Additionally, other reinforcement learning algorithms could also be tried to see if they can achieve better performance on this task."
      ],
      "metadata": {
        "id": "7zZAdBrRt9OP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "oweWhzamt9j-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}