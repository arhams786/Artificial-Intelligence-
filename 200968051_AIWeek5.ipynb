{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MOHD ARHAM SHAIKH\n",
        "#200968051\n",
        "#WEEK5"
      ],
      "metadata": {
        "id": "sKzZ6hQY-5a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Tensorlow Agents"
      ],
      "metadata": {
        "id": "EqfL1PdZGKWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting tf-agents\n  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\nCollecting pygame==2.1.0\n  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nCollecting gym<=0.23.0,>=0.17.0\n  Downloading gym-0.23.0.tar.gz (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\nRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\nRequirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\nRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697660 sha256=1278d917f82312c13f2a5a12ae2627786fce2b099764af657512e044e5d5095a\n  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\nSuccessfully built gym\nInstalling collected packages: pygame, gym, tf-agents\n  Attempting uninstall: gym\n    Found existing installation: gym 0.25.2\n    Uninstalling gym-0.25.2:\n      Successfully uninstalled gym-0.25.2\nSuccessfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.15.0\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:05.048966Z",
          "iopub.status.busy": "2023-02-16T13:49:05.048434Z",
          "iopub.status.idle": "2023-02-16T13:49:11.778121Z",
          "shell.execute_reply": "2023-02-16T13:49:11.777323Z"
        },
        "id": "KEHR2Ui-lo8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdb71f2-4489-42d1-f9e6-4cf9d5f222c3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "O7gLdUS6b2EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:11.781985Z",
          "iopub.status.busy": "2023-02-16T13:49:11.781745Z",
          "iopub.status.idle": "2023-02-16T13:49:14.398000Z",
          "shell.execute_reply": "2023-02-16T13:49:14.397322Z"
        },
        "id": "3oCS94Z83Jo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 -Create a environment \n",
        "\n"
      ],
      "metadata": {
        "id": "BWp9mCiyDlxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Observation between 5 and -5, Actions in (0,1,2) \n",
        "For  which  the  observation  is  a  random  integer  between -5 and 5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation."
      ],
      "metadata": {
        "id": "FVm5WPn6HXBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiArmedBanditEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self):\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._episode_ended = False\n",
        "    self._observation = None\n",
        "    self._reward = None\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._observation = np.random.randint(low=-5, high=6)\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    self._reward = self._observation * action\n",
        "    self._episode_ended = True\n",
        "    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:14.402126Z",
          "iopub.status.busy": "2023-02-16T13:49:14.401753Z",
          "iopub.status.idle": "2023-02-16T13:49:14.408345Z",
          "shell.execute_reply": "2023-02-16T13:49:14.407788Z"
        },
        "id": "TTaG2ZapQvHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Actions on Observation Sign (0 is -ve, 2 if +ve)\n",
        "Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive."
      ],
      "metadata": {
        "id": "Rl6d2IlDDumq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_policy(observation):\n",
        "  if observation < 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:14.411603Z",
          "iopub.status.busy": "2023-02-16T13:49:14.411056Z",
          "iopub.status.idle": "2023-02-16T13:49:14.415544Z",
          "shell.execute_reply": "2023-02-16T13:49:14.415002Z"
        },
        "id": "YV6DhsSi227-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c. Reward for 50 Observations\n",
        "Request  for 50 observations  from  the  environment, compute  and  print the total reward."
      ],
      "metadata": {
        "id": "V_yn0HTSDyXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = MultiArmedBanditEnv()\n",
        "total_reward = 0\n",
        "\n",
        "for _ in range(50):\n",
        "  time_step = env.reset()\n",
        "  action = optimal_policy(time_step.observation)\n",
        "  time_step = env.step(action)\n",
        "  total_reward += time_step.reward\n",
        "\n",
        "print('Total reward:', total_reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward: 170.0\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:14.418196Z",
          "iopub.status.busy": "2023-02-16T13:49:14.417987Z",
          "iopub.status.idle": "2023-02-16T13:49:14.422821Z",
          "shell.execute_reply": "2023-02-16T13:49:14.422212Z"
        },
        "id": "Eo_uwSz2gAKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f105ab36-7c6d-44a9-f4b8-eecc9d4ba653"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 –Create an environment \n"
      ],
      "metadata": {
        "id": "cDCwIKyaD2tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Two types of reward\n",
        "Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized."
      ],
      "metadata": {
        "id": "HqMugALoHYlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self, reward_sign):\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._episode_ended = False\n",
        "    self._observation = None\n",
        "    self._reward = None\n",
        "    self._reward_sign = reward_sign\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._observation = np.random.randint(low=-5, high=6)\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._reward_sign == 'original':\n",
        "      self._reward = self._observation * action\n",
        "    else:\n",
        "      self._reward = -self._observation * action\n",
        "\n",
        "    self._episode_ended = True\n",
        "    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:14.436531Z",
          "iopub.status.busy": "2023-02-16T13:49:14.435951Z",
          "iopub.status.idle": "2023-02-16T13:49:14.440993Z",
          "shell.execute_reply": "2023-02-16T13:49:14.440447Z"
        },
        "id": "VpMZlplNK5ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Different Cases of Policy\n",
        "Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle:\n",
        "\n",
        "##### i.The agent has not detected know yet which version of the environment is running.\n",
        "\n",
        "##### ii.The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
        "\n",
        "##### iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running."
      ],
      "metadata": {
        "id": "zRMV-BISD-3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "  def __init__(self):\n",
        "    self._state = 'unknown'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    if self._state == 'unknown':\n",
        "      if observation >= 0:\n",
        "        self._state = 'original'\n",
        "        return 2\n",
        "      else:\n",
        "        self._state = 'flipped'\n",
        "        return 0\n",
        "    elif self._state == 'original':\n",
        "      return 2\n",
        "    else:\n",
        "      return 0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:14.443499Z",
          "iopub.status.busy": "2023-02-16T13:49:14.443278Z",
          "iopub.status.idle": "2023-02-16T13:49:18.010689Z",
          "shell.execute_reply": "2023-02-16T13:49:18.010029Z"
        },
        "id": "Z0_5vMDCVZWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### c. Agent Detecting sign\n",
        "Define the agent that detects the sign of the environment and sets the policy appropriately."
      ],
      "metadata": {
        "id": "vdbWedcCEFEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self._policy = Policy()\n",
        "\n",
        "  def update_policy(self, reward_sign):\n",
        "    if reward_sign == 'original':\n",
        "      self._policy._state = 'original'\n",
        "    else:\n",
        "      self._policy._state = 'flipped'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    return self._policy.get_action(observation)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-16T13:49:18.013880Z",
          "iopub.status.busy": "2023-02-16T13:49:18.013647Z",
          "iopub.status.idle": "2023-02-16T13:49:18.020580Z",
          "shell.execute_reply": "2023-02-16T13:49:18.020020Z"
        },
        "id": "CiB935of-wVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here's an example output for each case:\n",
        "\n"
      ],
      "metadata": {
        "id": "JmCeC2DqEWqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Case 1: Original Environment (reward = observation * action)"
      ],
      "metadata": {
        "id": "WRa49ErAHTRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='original')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('original')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward:  20.0\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5aGN_rtDfSu",
        "outputId": "b7dc1884-c3dc-499f-9dec-1dc7fdd70ac3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward should be positive since the optimal policy will result in positive rewards for positive observations and negative rewards for negative observations."
      ],
      "metadata": {
        "id": "1dbgMTxIFeld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Case 2: Flipped Environment (reward = -observation * action)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jf-klQCpFkN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='flipped')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('flipped')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward:  0.0\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU5zmF0NEZLK",
        "outputId": "f9daa1b1-89bd-4479-f24f-464b4dc83e2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines an environment where the reward is always 0, regardless of the action and observation. Therefore, the total reward in this environment should always be 0."
      ],
      "metadata": {
        "id": "l3bi4Wh2Focs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8e840bfde2c9dbf1d829a22db5c55a245aabb8fa1c429845aff4444f12ec020"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}